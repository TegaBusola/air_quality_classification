# air_quality_classification

3. Methodology 

In this section, we will explore the methodology used to analyse the Air Quality categories within the dataset. The analysis in this report relies on Python and its related machine learning libraries for comprehensive data processing and modelling. 

3.1 Data Sources and Collection 

The quality of the dataset was found to be subpar, necessitating the decision to eliminate data to improve the accuracy of the machine learning algorithms. Concerns arose in several cities due to missing values, casting doubt on the dataset's reliability for drawing conclusions and making credible recommendations. Moreover, questions about the dataset's validity surfaced, especially when consistent days showed air pollutant values recorded as 0, a situation inconsistent with the reality of air pollution in any country. These anomalies pointed toward potential manual data entry rather than automated collection from pollutant monitoring stations. This suspicion was substantiated by the confirmation that some monitoring stations in India still rely on manual data entry and collection (Deshpande 2021) thereby affirming the hypothesis. 

3.2 Data Processing 

The data processing phase corresponds to the data understanding and processing stages outlined in the CRISP-DM model (Appendix 4). During this phase, a meticulous review of the raw data was conducted to comprehend the attributes and tuples thoroughly. This process involved verifying data types and making critical decisions to maintain data integrity, ensure quality, and validate the dataset. Notably, the decision was made to exclude rows with missing values before applying machine learning algorithms. This strategic choice, as opposed to computing averages, was guided by several advantages, including the preservation of data integrity, maintenance of sample size, smoothing of variability, retention of context, and alignment with research objectives. Despite the reduction in dataset size, this approach enabled the preservation of the overall data structure, preventing the loss of valuable information and ensuring a realistic representation of the dataset within the research context. 

It is essential to acknowledge the potential drawbacks associated with this approach, including the risk of introducing biases, overfitting issues, and working with a reduced dataset (Bhandari 2022). Consequently, this report recognizes that the presented results might not be entirely devoid of inaccuracies due to the values removed. However, the careful consideration of these factors highlights the methodological choices made and provides transparency regarding the limitations of the analysis. 

3.3 Feature Selection 

Feature selection is a crucial step in building an effective machine learning model. The goal is to identify and include the most relevant features that contribute significantly to the predictive power of the model. 

Features directly related to the target variable are chosen. In the context of predicting Air Quality Index (AQI) categories, pollutants such as particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), sulfur dioxide (SO2), carbon monoxide (CO), and ozone (O3) are highly relevant. Features highly correlated with the target variable are selected. High correlation indicates a strong relationship and suggests that changes in the feature are associated with changes in the target. Tools like correlation matrices or heatmaps help visualise these relationships. By combining these criteria and methods, the features that contribute most significantly to the predictive accuracy of the model, ensuring a robust and interpretable solution for the given problem, such as predicting AQI categories. 

3.4 Model Selection 

The selection of an appropriate regression model was pivotal for the accurate prediction of AQI values within the dataset. Among the array of supervised classifier algorithms, Naïve Bayes emerges as a noteworthy contender. This algorithm relies on Bayes' Theorem, calculating the probability of a sample belonging to a specific class based on its features (Chauhan 2022). In the context of this report, Naïve Bayes was chosen as the benchmark model for several reasons. Notably, its compatibility with categorical data, such as the AQI categories, coupled with its effectiveness in addressing multiclass prediction challenges, rendered it a fitting choice. 

However, it is imperative to acknowledge the limitations inherent to the Naïve Bayes model. Assumptions, such as the independence of features, potential class imbalances, and challenges in handling missing data, have been identified in prior research (Bathula 2023). In light of these limitations, to mitigate the model's shortcomings and validate its predictions, Naïve Bayes was assessed against a diverse array of models. These included, Random Forest, C4.5 Decision Trees, XGBoost, and Support Vector Regression. By subjecting Naïve Bayes to this comprehensive comparative analysis, this research aims to scrutinize its predictive validity and identify potential blind spots in forecasting AQI categories. 

3.5 Model Training 

The chosen classification algorithms were applied to a systematically partitioned dataset. The partitioning strategy employed a precise 60-20-20 split, where 60% of the data served the purpose of training the machine learning models. The subsequent 20% was designated for both validating and fine-tuning hyperparameters, ensuring a meticulous evaluation process. The final 20% of the dataset was exclusively reserved for rigorously assessing the performance of the fully developed model.  

Notably, while specific hyperparameters were not explicitly set, a robust approach was undertaken. The k-fold cross-validation method was employed, conducting a rigorous 10-fold cross-validation. This methodological choice was instrumental in assessing the model's ability to generalise effectively to an independent dataset, ensuring the reliability and integrity of the experimental results. 

3.6 Model Evaluation Metrics 

In assessing the performance of the selected models and facilitating comparisons, diverse evaluation metrics were employed. Mean accuracy, within the context of cross-validation, denotes the average accuracy of a machine learning model across multiple folds or subsets of the dataset. To accomplish this, a 10-fold cross-validation was conducted, partitioning the dataset into ten subsets or folds. The model was iteratively trained and evaluated ten times, utilising a distinct fold as the test set during each iteration while employing the remaining data for training. Subsequently, individual accuracy scores from each fold were averaged to calculate the mean accuracy. 

Moreover, the model's accuracy on the untouched test dataset, uninvolved in both training and cross-validation, was determined. This metric signifies the ratio of correctly predicted labels to the total number of samples in the test set. Additionally, a comprehensive classification report was generated, offering a detailed account of the model's performance for each class within the dataset. This report incorporated crucial metrics such as precision, recall, and F1-score for individual classes, alongside an overall accuracy assessment. 

3.7 Implementation Details 

Python was the language of choice for implementing the models, owing to its simplicity and consistent usability for both experienced and novice coders (Kvartalyni 2022). Python's strength in machine learning found expression through libraries like Scikit-learn (Sklearn), a cornerstone of every model in this study. Sklearn, a robust machine learning library, offers an array of classification, regression, and clustering algorithms (Vadapalli 2020). Furthermore, the visualisation of model evaluation metrics was enhanced using the Matplotlib and Seaborn libraries, ensuring clear and concise representation. 

3.8 Assumptions and Limitations 

Drawing upon prior studies spanning the past three decades on India's air quality, an assumption of the AQI category as “poor” was made regarding the gradual, rapid decline in air quality. This assumption was influenced by the bias present in the data due to the aggregation of pollutant monitoring stations in large Indian cities. Notably, megacities possessed a disproportionate number of stations in contrast to rural or smaller cities, where monitoring stations were scarce or non-existent. 

As noted earlier, missing values in the dataset, including AQI categories, were addressed through exclusion, resulting in the removal of 16% of the raw data. A significant limitation arose from the imbalanced distribution of AQI categories, with the "Moderate" and "Satisfactory" categories collectively constituting less than 60% (excluding missing values) of the dataset. This skewed distribution posed a risk of overfitting or misclassification, particularly in predicting instances as "Moderate" or "Satisfactory". These limitations were carefully considered throughout the analysis, acknowledging their potential impact on the study's outcomes. 

In the context of predicting the Air Quality Index (AQI) categorisation for India, Naive Bayes served as the benchmark model, achieving an accuracy rate of 84.78%. This performance was achieved despite the inherent challenges posed by class imbalance within the dataset. Particularly noteworthy was the influence of the "Moderate" and "Satisfactory" categories, which constituted less than 60% of the raw data. These categories significantly impacted the model's efficacy. However, Naive Bayes demonstrated a commendable equilibrium in precision, recall, and F1-score, showcasing its ability to handle the skewed class distribution effectively. 

In contrast, C4.5 Decision Tree, outperformed the Naive Bayes benchmark model with an impressive accuracy rate of 99.92%. Remarkably, this accuracy was maintained even in the presence of class imbalance. The success of C4.5 implies that the uneven distribution of classes did not substantially impair the model's accuracy in predicting AQI categories. This remarkable performance could be attributed to meticulous pruning techniques that helped prevent overfitting, rendering C4.5 a robust choice for AQI prediction under conditions of skewed class distribution.  

Random Forest emerged as a standout performer, achieving a remarkable accuracy rate of 99.84%, surpassing the benchmark set by Naive Bayes at 84.78%. This achievement underscored Random Forest's resilience in the face of class imbalance. By amalgamating multiple decision trees, the algorithm adeptly captured intricate relationships within air quality data, effectively overcoming the challenges posed by uneven category representation. A crucial factor contributing to this success was the meticulous tuning of hyperparameters, which played a pivotal role in enabling the model to navigate the skewed class distribution effectively. 

Similar to both C4.5 and Random Forest, SVM demonstrated exceptional performance with an accuracy of 99.44%, even in the context of class imbalance. Its proficiency in handling high-dimensional data and capturing complex patterns likely contributed significantly to mitigating the impact of the skewed class distribution. The thoughtful selection of kernel functions and regularisation parameters further bolstered the model's robustness under conditions of class imbalance, highlighting SVM's effectiveness in addressing such challenges. 

XGBoost, an advanced gradient boosting algorithm, emerged as a formidable contender, exhibiting outstanding performance with an impressive accuracy rate of 99.52%. This achievement can be attributed to its ensemble of decision trees, meticulously fine-tuned through hyperparameter adjustments, enabling the model to discern subtle patterns within air quality variables. This success highlights the critical role of parameter optimization in addressing data imbalances effectively. 

Preprocessing techniques, including feature scaling, handling missing values, and categorical variable encoding, assumed pivotal significance, especially in light of the inherent class imbalance. Feature standardisation played a crucial role in ensuring unbiased influence, preventing skewed variables from unduly influencing predictions. Skilful management of missing values preserved essential data integrity, while categorical variable encoding augmented the models' comprehension of non-numeric variables. These preprocessing steps bolstered the models' predictive prowess amidst class imbalances. 

It is imperative to recognise that each algorithm employs distinct data processing features. Hence, the selection of an appropriate comparison method holds paramount importance. Mere reliance on a parameter with a high value, signifying a superior metric score, can be misleading and potentially indicative of "statistical bias or misdirected metric design" (Ghosh, 2023). To mitigate such pitfalls, a t-test, a robust statistical tool which compares the means of two samples, was employed in this report. Specifically, the t-test was conducted to compare the means of the Naïve Bayes algorithm against C4.5, Random Forest, Support Vector Machine (SVM), and XGBoost, ensuring a rigorous and unbiased evaluation of their respective performances. 

The outcomes of the t-tests reveal substantial disparities in accuracy scores between Naive Bayes and its counterparts (C4.5, Random Forest, XGBoost, and SVM). The negative t-statistics signify that, on average, Naive Bayes lags behind the other models in terms of accuracy. Furthermore, the remarkably small p-values (all rounded to 0.01) provide compelling evidence against the null hypothesis, indicating the statistical significance of these disparities. The t-tests unequivocally demonstrate that Naive Bayes underperforms in accuracy compared to C4.5, Random Forest, XGBoost, and SVM for the specific dataset and accuracy values under consideration. 

ANOVA, an acronym for Analysis of Variance, serves as a powerful statistical tool for discerning differences among means across three or more independent groups (Bevens 2023). When applied to our models, it yielded an F-statistic of 96.73, signifying that the variation between algorithms' means is 96.73 times greater than the variation within each individual group. A high F-statistic of this magnitude strongly suggests significant disparities among at least some of the groups' means. The corresponding p-value, an infinitesimal 0.0, unequivocally underscores the statistical significance of the differences in accuracy scores among the machine learning models. Consequently, it is reasonable to conclude that at least one of the models performs markedly distinct from the others in terms of accuracy. 

Both ANOVA and the t-tests are consistent in their assessment concerning the Naive Bayes algorithm. The t-tests reveal noteworthy differences in accuracy between Naive Bayes and its counterparts (C4.5, Random Forest, SVM, and XGBoost), supported by exceedingly low p-values (0.01). Similarly, the ANOVA test underscores significant variations among the means of the models, substantiated by an extremely low p-value (0.0). 

In summary, the convergence of findings from both ANOVA and the t-tests solidifies the conclusion that Naive Bayes diverges significantly from other machine learning models in terms of accuracy. The consistent and robust evidence against the null hypothesis reinforces the notion of substantial differences in accuracy scores among the models, affirming the distinct performance of Naive Bayes in this context. 
